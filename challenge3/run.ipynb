{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19244f09910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE TRAIN AND VAL\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "val = pd.read_csv('data/val.csv')\n",
    "tPath = list(train.ImageID)\n",
    "vPath = list(val.ImageID)\n",
    "tPathOut = ['train/'+str(i) for i in tPath]\n",
    "vPathOut = ['val/'+str(i) for i in vPath]\n",
    "tPathOut.extend(vPathOut)\n",
    "labels = list(train.label)\n",
    "labels.extend(list(val.label))\n",
    "train_ = pd.DataFrame({'ImageID':tPathOut,'label':labels})\n",
    "train_.to_csv('data/trainAndVal.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type         | Params\n",
      "----------------------------------------\n",
      "0 | resnet | EfficientNet | 18.2 M\n",
      "----------------------------------------\n",
      "18.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.2 M    Total params\n",
      "72.776    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d234008d28c840e5a7164e6ce83064a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04032e7258d410d995dbf1e47c88ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import model\n",
    "import albumentations as A\n",
    "from albumentations.augmentations.transforms import Flip\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "img_size = 230\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ckpt = pl.callbacks.model_checkpoint.ModelCheckpoint(monitor = 'val_mse',save_top_k = 1,mode = 'min')\n",
    "    trainer = Trainer(max_epochs = 50,gpus = 1, callbacks = ckpt, precision=16, amp_level='O1',deterministic=True,fast_dev_run = False)\n",
    "    \n",
    "    train_tr = A.Compose([\n",
    "        A.CenterCrop(img_size,img_size,always_apply=True),\n",
    "        Flip(p=0.25)\n",
    "    ])\n",
    "    \n",
    "    val_tr = A.Compose([\n",
    "        A.CenterCrop(img_size,img_size,always_apply=True)\n",
    "    ])\n",
    "    \n",
    "    model = model.Classifier({'lr':3e-4,'batch_size':48,'train_tr':train_tr,'val_tr':val_tr})\n",
    "    \n",
    "    #ckpt = torch.load(path)\n",
    "    #model.load_state_dict(ckpt['state_dict'])\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba8ac2e3f86443089ec9b03c8349efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import model\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.augmentations.transforms import Flip, VerticalFlip, HorizontalFlip\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "m = 0\n",
    "\n",
    "tr0 = A.Compose([\n",
    "        A.CenterCrop(230,230,always_apply=True)\n",
    "])\n",
    "\n",
    "tr1 = A.Compose([\n",
    "    A.CenterCrop(230,230,always_apply=True),\n",
    "    A.VerticalFlip(p = 1)\n",
    "])\n",
    "\n",
    "tr2 = A.Compose([\n",
    "    A.CenterCrop(230,230,always_apply=True),\n",
    "    A.HorizontalFlip(p = 1)\n",
    "])\n",
    "\n",
    "tr3 = A.Compose([\n",
    "    A.CenterCrop(230,230,always_apply=True),\n",
    "    A.HorizontalFlip(p = 1),\n",
    "    A.VerticalFlip(p = 1)\n",
    "])\n",
    "\n",
    "tr_list = [tr0,tr1,tr2,tr3]\n",
    "concat = torch.tensor([])\n",
    "\n",
    "for tr in tr_list:\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        path = '' #PATH OF LAST CHECKPOINT\n",
    "        trainer = Trainer(gpus = 1, precision=16, amp_level='O1',deterministic=True)\n",
    "\n",
    "        train_tr = A.Compose([\n",
    "            A.CenterCrop(230,230,always_apply=True)\n",
    "        ])\n",
    "\n",
    "        val_tr = A.Compose([\n",
    "            A.CenterCrop(230,230,always_apply=True)\n",
    "        ])\n",
    "\n",
    "        m = model.Classifier({'lr':3e-4,'batch_size':48,'train_tr':train_tr,'val_tr':tr})\n",
    "\n",
    "        ckpt = torch.load(path)\n",
    "        m.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        out = trainer.predict(m)\n",
    "        output = torch.tensor([])\n",
    "\n",
    "        for i in range(len(out)):\n",
    "            output = torch.cat((output,torch.tensor(out[i]).argmax(1)))\n",
    "        concat = torch.cat((concat,output.unsqueeze(0)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.round(concat.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "model.writeSub(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
